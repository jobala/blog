<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <link rel="stylesheet" href="/css/reset.css" />
  <link rel="stylesheet" href="/css/font.css" />
  <link rel="stylesheet" href="/css/butterick.css" />
  <script defer language="javascript" type="text/javascript" src="/js/gh.js"></script>
  <script src="https://unpkg.com/github-calendar@latest/dist/github-calendar.min.js"></script>
  <link rel="stylesheet" href="https://unpkg.com/github-calendar@latest/dist/github-calendar-responsive.css" />
  <link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">  
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
</head>

  <body>
    <header>
  <nav>
    <div class="home">
      <a href="http://localhost:1313/">
        <h3>Home</h3>
      </a>
    </div>
    
    
      
        
        <div class="vl"></div>
        <div class="menu-item">
          <a  href="/about/"><h3>About</h3></a>
        </div>
      
    
  </nav>
</header>

    <div id="content">
      
  <h1>Scheduling</h1>
  <p>In <code>kernel/main.c</code> we see that once xv6 has booted, it sets up a process table with a call to procinit.</p>


<pre class="gh"> https://github.com/mit-pdos/xv6-riscv/blob/de247db5e6384b138f270e0a7c745989b5a9c23b/kernel/main.c#L22-L21 </pre>


<p>It is necessary to setup the process table before calling <code>scheduler</code> because the <code>scheduler</code> needs a process table from which it can select a
process to give CPU time to.</p>


<pre class="gh"> https://github.com/mit-pdos/xv6-riscv/blob/de247db5e6384b138f270e0a7c745989b5a9c23b/kernel/proc.c#L46-L59 </pre>


<p>In <code>procinit</code> we initialize an array with 64 elements, this array is our process table and by definition it contains a
maximum of 64 processes. We then acquire pid and wait locks, which I am not sure why we do that because at this point we
are still running on a single core and there&rsquo;s no scheduling yet, anyway, the pid lock ensures that the pid are assigned
consistently and I am not sure what the wait lock does. The process state is then set to <code>UNUSED</code> and we initilize the
process kernel stack <code>kstack</code>.</p>
<p>And calls the <code>scheduler</code> function to start running the scheduler. Note
that the process table is only setup by the booting core and the schedule function is executed by all cores.</p>


<pre class="gh"> https://github.com/mit-pdos/xv6-riscv/blob/de247db5e6384b138f270e0a7c745989b5a9c23b/kernel/main.c#L44-L43 </pre>


<p>When each CPU core loads, it starts executing the schedular which is a non terminating program, running an infinite
loop. The scheduler picks a waiting process from a process table and switches control to it. This switching is known
appropriately as context switching.</p>


<pre class="gh"> https://github.com/mit-pdos/xv6-riscv/blob/de247db5e6384b138f270e0a7c745989b5a9c23b/kernel/proc.c#L444-L481 </pre>


<p>In essence, <code>scheduler</code> looks through the process table for a process in the <code>RUNNABLE</code> state, updates its state to
<code>RUNNING</code> and then calls <code>swtch</code>.  Calling <code>swtch</code> starts running that process and when the process is done running, it
switches back to <code>scheduler</code>. This switching is appropriately called context switching. The <code>swtch</code> method swaps the
context of the core with that of the process. The context being a set of registers.</p>


<pre class="gh"> https://github.com/mit-pdos/xv6-riscv/blob/de247db5e6384b138f270e0a7c745989b5a9c23b/kernel/proc.h#L2-L19 </pre>


<p>The <code>ra</code> register holds the address from which the cpu resumes executation and <code>sp</code> points to the stack so that the
process resumes with the same stack values it had when it was paused.</p>
<h2 id="pre-emptive-multitasking">Pre-emptive Multitasking</h2>
<p>Some CPU bound processes take long to computer and have to be asked to kindly leave the CPU for other processes to run.
This is how pre-emptive scheduling works and the mechanism is made possible by <strong>timer interrupts</strong>. Timer interrupts
are generated by a timer hardware.</p>
<p>When a timer interrupt happens control jumps to <code>usertrap</code> where <code>yield</code> is called if we are handling a timer interrupt.


<pre class="gh"> https://github.com/mit-pdos/xv6-riscv/blob/de247db5e6384b138f270e0a7c745989b5a9c23b/kernel/trap.c#L79-L81 </pre>

</p>
<p>Yield acquires a lock over the process then update the process&rsquo; state to <code>RUNNABLE</code> which means another cpu core can run
it once the lock is released. Before releasing the process lock, <code>sched</code> is invoked.</p>


<pre class="gh"> https://github.com/mit-pdos/xv6-riscv/blob/de247db5e6384b138f270e0a7c745989b5a9c23b/kernel/proc.c#L510-L519 </pre>


<p><code>sched</code> doesn&rsquo;t do much, other than calling <code>swtch</code>. This time restoring the registers in the <strong>cpu context</strong> where the
return address is the address after the call to <code>swtch</code> in <code>schedule</code>. That is to say, <code>yield</code> gives back control to
<code>schedule</code> so that it can find another process and run it.</p>


<pre class="gh"> https://github.com/mit-pdos/xv6-riscv/blob/de247db5e6384b138f270e0a7c745989b5a9c23b/kernel/proc.c#L490-L508 </pre>




    </div>
    
  </body>
  <script>
    
    GitHubCalendar(".calendar", "jobala", { responsive: true });
  </script>
</html>
